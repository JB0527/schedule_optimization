{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing - MLP"
      ],
      "metadata": {
        "id": "CtbQbJQbqsSF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hand-crafted filtering, data distribution 을 이용한 filtering 방식과 다르게 공정 시작 시간을 이용해서 작업과 작업 사이의 쉬는 시간을 파악해 작업 시간을 직접 조정한다."
      ],
      "metadata": {
        "id": "GVnb8srsqzAc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5 data는 공정 시작 시간, 공정 완료 시간이 존재하지만 나머지 step들에 대해서는 공정 시작 시간 밖에 존재하지 않는다. 그래서 우리는 다음 파이프의 공정 시작 시간에서 현재 파이프의 공정 시작 시간의 차이를 작업 시간으로 간주했지만 이는 실질적으로 공정 시작, 완료 시간 사이의 쉬는 시간에 대해서는 고려하지 못하기 때문에 정확한 공정 시간이라고 할 수 없고 실제로 regression 수행 시 큰 error 값을 가진다."
      ],
      "metadata": {
        "id": "UFf1wYgWrP--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "이를 위해 기존의 data에서 outlier를 filtering 하는 방식보다는 작업 시간을 조정해주는 방법을 고려해 보았다.\n"
      ],
      "metadata": {
        "id": "jvO1djtZry5t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "먼저 Step5 data에 대해서 공정 시작 시간을 기준으로 오름차순을 해준다.\n",
        "\n",
        "MLP의 input은 $i$ 번째 파이프와 $i+1$ 번째 파이프의 공정 시작 시간이다.\n",
        "Output은 $i+1$ 번째 파이프의 공정 시작 시간과 $i$ 번째 파이프의 공정 완료 시간 즉, 현재 파이프와 다음 파이프 공정 사이의 쉬는 시간을 의미한다.\n",
        "\n",
        "만약 모델이 잘 학습 된다면 $i$ 번째 파이프와 $i+1$ 번째 파이프의 공정 시작 시간으로 이 두 파이프 공정 사이의 쉬는 시간을 알 수 있을 것이고 우리가 기존에 사용했던\n",
        "\n",
        " $$작업\\ 시간_i = i+1\\ 번째\\ 파이프의\\ 공정\\ 시작\\ 시간 - i\\ 번째\\ 파이프의\\ 공정\\ 시작\\ 시간$$\n",
        "\n",
        "에서 MLP output 값을 빼주면 $i$ 번째 파이프의 공정 시간에서 쉬는 시간까지 고려해 조정할 수 있을 것이다."
      ],
      "metadata": {
        "id": "3Big8wAps-Rd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "$$작업\\ 시간_i = i+1\\ 번째\\ 파이프의\\ 공정\\ 시작\\ 시간 - i\\ 번째\\ 파이프의\\ 공정\\ 시작\\ 시간 - \\text{MLP}의\\ \\text{output}\\ (쉬는\\ 시간)$$"
      ],
      "metadata": {
        "id": "mmomFDqHvJsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Work directory 이동\n",
        "%cd /content/drive/MyDrive/pipe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-uMuoGJ11gqQ",
        "outputId": "4ee7a99a-14c7-41ae-da8e-182bee65b8c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/pipe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p64gkal70el_"
      },
      "outputs": [],
      "source": [
        "# Library import\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import DataLoader, TensorDataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_excel('regression.xlsx')\n",
        "\n",
        "# [input1, input2], output\n",
        "X = data[['new_input1', 'new_input2']].values  # new_input은 기존의 timestampe type의 input을 int type으로 바꿔준 형태이다.\n",
        "y = data['output'].values  # output : 쉬는 시간 (= 다음 파이프의 시작 시간 - 현재 파이프의 완료 시간)\n",
        "\n",
        "\n",
        "# Data scaling (StandardScaler)\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)\n",
        "\n",
        "\n",
        "# Data -> Tensor\n",
        "X_train_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "\n",
        "# DataLoader\n",
        "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)"
      ],
      "metadata": {
        "id": "-cB0x3A-0sW3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP Model\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.layer1 = nn.Linear(2, 64)  # input layer (2 features)\n",
        "        self.layer2 = nn.Linear(64, 64)  # hidden layer\n",
        "        self.output_layer = nn.Linear(64, 1)  # output layer (1 output)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.layer1(x))\n",
        "        x = torch.relu(self.layer2(x))\n",
        "        x = self.output_layer(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "L1pqDmcS0753"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model, Optimizer, Loss\n",
        "model = MLP()\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "ADb6hs7Z0715",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cdeaf6e-b726-4eba-daaa-b95fad5a063b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLP(\n",
              "  (layer1): Linear(in_features=2, out_features=64, bias=True)\n",
              "  (layer2): Linear(in_features=64, out_features=64, bias=True)\n",
              "  (output_layer): Linear(in_features=64, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Model Train\n",
        "def train(model, train_loader, criterion, optimizer, epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, targets in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs.to(device))\n",
        "            loss = criterion(outputs, targets.to(device))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(train_loader):.4f}')"
      ],
      "metadata": {
        "id": "E34y-bmC1ZtA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Train!!\n",
        "train(model, train_loader, criterion, optimizer, epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwEAYZ281Zq6",
        "outputId": "2f2e64ad-de16-4484-9f13-ca73c0ade503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/1000], Loss: 1234616.1427\n",
            "Epoch [2/1000], Loss: 1234618.1716\n",
            "Epoch [3/1000], Loss: 1234758.4787\n",
            "Epoch [4/1000], Loss: 1234115.8781\n",
            "Epoch [5/1000], Loss: 1233801.8856\n",
            "Epoch [6/1000], Loss: 1233501.1005\n",
            "Epoch [7/1000], Loss: 1233188.5448\n",
            "Epoch [8/1000], Loss: 1232935.0287\n",
            "Epoch [9/1000], Loss: 1232779.7470\n",
            "Epoch [10/1000], Loss: 1233110.2922\n",
            "Epoch [11/1000], Loss: 1232663.7523\n",
            "Epoch [12/1000], Loss: 1232419.9566\n",
            "Epoch [13/1000], Loss: 1232352.4330\n",
            "Epoch [14/1000], Loss: 1232217.2509\n",
            "Epoch [15/1000], Loss: 1232219.6423\n",
            "Epoch [16/1000], Loss: 1232015.6219\n",
            "Epoch [17/1000], Loss: 1231893.2074\n",
            "Epoch [18/1000], Loss: 1231927.4129\n",
            "Epoch [19/1000], Loss: 1231597.1921\n",
            "Epoch [20/1000], Loss: 1231422.0630\n",
            "Epoch [21/1000], Loss: 1231233.8469\n",
            "Epoch [22/1000], Loss: 1231058.0428\n",
            "Epoch [23/1000], Loss: 1230803.0114\n",
            "Epoch [24/1000], Loss: 1230571.3209\n",
            "Epoch [25/1000], Loss: 1230378.4304\n",
            "Epoch [26/1000], Loss: 1230050.5474\n",
            "Epoch [27/1000], Loss: 1229749.8748\n",
            "Epoch [28/1000], Loss: 1229422.0366\n",
            "Epoch [29/1000], Loss: 1229055.1728\n",
            "Epoch [30/1000], Loss: 1228684.1592\n",
            "Epoch [31/1000], Loss: 1228698.1356\n",
            "Epoch [32/1000], Loss: 1228387.4651\n",
            "Epoch [33/1000], Loss: 1231963.2949\n",
            "Epoch [34/1000], Loss: 1226984.5131\n",
            "Epoch [35/1000], Loss: 1226425.7540\n",
            "Epoch [36/1000], Loss: 1226064.2807\n",
            "Epoch [37/1000], Loss: 1225424.1108\n",
            "Epoch [38/1000], Loss: 1224904.0902\n",
            "Epoch [39/1000], Loss: 1224202.2937\n",
            "Epoch [40/1000], Loss: 1223724.0251\n",
            "Epoch [41/1000], Loss: 1222915.6588\n",
            "Epoch [42/1000], Loss: 1222594.6068\n",
            "Epoch [43/1000], Loss: 1221565.8883\n",
            "Epoch [44/1000], Loss: 1220883.5613\n",
            "Epoch [45/1000], Loss: 1220073.5290\n",
            "Epoch [46/1000], Loss: 1219341.0050\n",
            "Epoch [47/1000], Loss: 1218447.6873\n",
            "Epoch [48/1000], Loss: 1217654.4245\n",
            "Epoch [49/1000], Loss: 1216756.8017\n",
            "Epoch [50/1000], Loss: 1215836.0386\n",
            "Epoch [51/1000], Loss: 1214972.9143\n",
            "Epoch [52/1000], Loss: 1213956.3391\n",
            "Epoch [53/1000], Loss: 1213329.2524\n",
            "Epoch [54/1000], Loss: 1211915.9192\n",
            "Epoch [55/1000], Loss: 1210827.7746\n",
            "Epoch [56/1000], Loss: 1209815.3444\n",
            "Epoch [57/1000], Loss: 1208844.4440\n",
            "Epoch [58/1000], Loss: 1207540.4411\n",
            "Epoch [59/1000], Loss: 1206316.8258\n",
            "Epoch [60/1000], Loss: 1205244.9506\n",
            "Epoch [61/1000], Loss: 1208634.0942\n",
            "Epoch [62/1000], Loss: 1202585.6711\n",
            "Epoch [63/1000], Loss: 1201243.0692\n",
            "Epoch [64/1000], Loss: 1200859.1358\n",
            "Epoch [65/1000], Loss: 1198665.8341\n",
            "Epoch [66/1000], Loss: 1197238.4808\n",
            "Epoch [67/1000], Loss: 1195848.7105\n",
            "Epoch [68/1000], Loss: 1194279.4634\n",
            "Epoch [69/1000], Loss: 1193084.8069\n",
            "Epoch [70/1000], Loss: 1191271.9228\n",
            "Epoch [71/1000], Loss: 1190205.7879\n",
            "Epoch [72/1000], Loss: 1188126.7572\n",
            "Epoch [73/1000], Loss: 1186533.6372\n",
            "Epoch [74/1000], Loss: 1184776.1598\n",
            "Epoch [75/1000], Loss: 1183577.8671\n",
            "Epoch [76/1000], Loss: 1181353.9527\n",
            "Epoch [77/1000], Loss: 1182372.5301\n",
            "Epoch [78/1000], Loss: 1177944.1947\n",
            "Epoch [79/1000], Loss: 1175974.7580\n",
            "Epoch [80/1000], Loss: 1174125.6407\n",
            "Epoch [81/1000], Loss: 1172768.0908\n",
            "Epoch [82/1000], Loss: 1170374.3120\n",
            "Epoch [83/1000], Loss: 1168354.8690\n",
            "Epoch [84/1000], Loss: 1166471.0261\n",
            "Epoch [85/1000], Loss: 1164271.4540\n",
            "Epoch [86/1000], Loss: 1162167.9902\n",
            "Epoch [87/1000], Loss: 1160037.2917\n",
            "Epoch [88/1000], Loss: 1158361.7911\n",
            "Epoch [89/1000], Loss: 1155848.2116\n",
            "Epoch [90/1000], Loss: 1153538.4200\n",
            "Epoch [91/1000], Loss: 1155592.4962\n",
            "Epoch [92/1000], Loss: 1148963.1591\n",
            "Epoch [93/1000], Loss: 1146631.3693\n",
            "Epoch [94/1000], Loss: 1144241.3399\n",
            "Epoch [95/1000], Loss: 1141819.2772\n",
            "Epoch [96/1000], Loss: 1139388.0079\n",
            "Epoch [97/1000], Loss: 1136863.0867\n",
            "Epoch [98/1000], Loss: 1134310.8630\n",
            "Epoch [99/1000], Loss: 1131854.1425\n",
            "Epoch [100/1000], Loss: 1129407.5833\n",
            "Epoch [101/1000], Loss: 1126527.3960\n",
            "Epoch [102/1000], Loss: 1124340.4085\n",
            "Epoch [103/1000], Loss: 1121129.6951\n",
            "Epoch [104/1000], Loss: 1118453.7152\n",
            "Epoch [105/1000], Loss: 1115692.5344\n",
            "Epoch [106/1000], Loss: 1113014.9390\n",
            "Epoch [107/1000], Loss: 1110228.9577\n",
            "Epoch [108/1000], Loss: 1107111.5557\n",
            "Epoch [109/1000], Loss: 1104207.5101\n",
            "Epoch [110/1000], Loss: 1101295.6901\n",
            "Epoch [111/1000], Loss: 1098242.9904\n",
            "Epoch [112/1000], Loss: 1095209.3333\n",
            "Epoch [113/1000], Loss: 1094084.3213\n",
            "Epoch [114/1000], Loss: 1089107.7814\n",
            "Epoch [115/1000], Loss: 1086007.8231\n",
            "Epoch [116/1000], Loss: 1082889.7668\n",
            "Epoch [117/1000], Loss: 1079681.1301\n",
            "Epoch [118/1000], Loss: 1076306.6032\n",
            "Epoch [119/1000], Loss: 1073013.2826\n",
            "Epoch [120/1000], Loss: 1069668.3124\n",
            "Epoch [121/1000], Loss: 1066382.5338\n",
            "Epoch [122/1000], Loss: 1062881.8461\n",
            "Epoch [123/1000], Loss: 1059618.7882\n",
            "Epoch [124/1000], Loss: 1056127.7490\n",
            "Epoch [125/1000], Loss: 1052586.3100\n",
            "Epoch [126/1000], Loss: 1049027.6195\n",
            "Epoch [127/1000], Loss: 1045304.8406\n",
            "Epoch [128/1000], Loss: 1041811.6852\n",
            "Epoch [129/1000], Loss: 1038106.6320\n",
            "Epoch [130/1000], Loss: 1034400.0674\n",
            "Epoch [131/1000], Loss: 1031028.3941\n",
            "Epoch [132/1000], Loss: 1026752.8254\n",
            "Epoch [133/1000], Loss: 1023361.0778\n",
            "Epoch [134/1000], Loss: 1019369.2874\n",
            "Epoch [135/1000], Loss: 1015637.5319\n",
            "Epoch [136/1000], Loss: 1011674.6403\n",
            "Epoch [137/1000], Loss: 1007670.1087\n",
            "Epoch [138/1000], Loss: 1003670.3481\n",
            "Epoch [139/1000], Loss: 999728.2721\n",
            "Epoch [140/1000], Loss: 995659.2658\n",
            "Epoch [141/1000], Loss: 991535.9260\n",
            "Epoch [142/1000], Loss: 988307.8129\n",
            "Epoch [143/1000], Loss: 983313.9991\n",
            "Epoch [144/1000], Loss: 979249.7448\n",
            "Epoch [145/1000], Loss: 975946.3198\n",
            "Epoch [146/1000], Loss: 970804.1819\n",
            "Epoch [147/1000], Loss: 966548.7365\n",
            "Epoch [148/1000], Loss: 962932.6181\n",
            "Epoch [149/1000], Loss: 958009.6940\n",
            "Epoch [150/1000], Loss: 953597.2286\n",
            "Epoch [151/1000], Loss: 949563.4973\n",
            "Epoch [152/1000], Loss: 944786.8165\n",
            "Epoch [153/1000], Loss: 940529.7829\n",
            "Epoch [154/1000], Loss: 937911.9881\n",
            "Epoch [155/1000], Loss: 931927.6251\n",
            "Epoch [156/1000], Loss: 926644.9206\n",
            "Epoch [157/1000], Loss: 922015.9814\n",
            "Epoch [158/1000], Loss: 917590.6617\n",
            "Epoch [159/1000], Loss: 1115011.8700\n",
            "Epoch [160/1000], Loss: 907173.7139\n",
            "Epoch [161/1000], Loss: 903542.2629\n",
            "Epoch [162/1000], Loss: 897731.9240\n",
            "Epoch [163/1000], Loss: 894259.7110\n",
            "Epoch [164/1000], Loss: 888383.7626\n",
            "Epoch [165/1000], Loss: 883905.7611\n",
            "Epoch [166/1000], Loss: 878819.1759\n",
            "Epoch [167/1000], Loss: 877407.1497\n",
            "Epoch [168/1000], Loss: 869317.5572\n",
            "Epoch [169/1000], Loss: 864552.3192\n",
            "Epoch [170/1000], Loss: 859365.1404\n",
            "Epoch [171/1000], Loss: 855216.6810\n",
            "Epoch [172/1000], Loss: 849495.7417\n",
            "Epoch [173/1000], Loss: 844505.9221\n",
            "Epoch [174/1000], Loss: 839537.2188\n",
            "Epoch [175/1000], Loss: 834610.0586\n",
            "Epoch [176/1000], Loss: 829489.8821\n",
            "Epoch [177/1000], Loss: 824491.8838\n",
            "Epoch [178/1000], Loss: 819271.0572\n",
            "Epoch [179/1000], Loss: 814124.1662\n",
            "Epoch [180/1000], Loss: 808659.2290\n",
            "Epoch [181/1000], Loss: 803827.9881\n",
            "Epoch [182/1000], Loss: 798526.1382\n",
            "Epoch [183/1000], Loss: 794764.4169\n",
            "Epoch [184/1000], Loss: 788554.7765\n",
            "Epoch [185/1000], Loss: 782826.8375\n",
            "Epoch [186/1000], Loss: 777281.4979\n",
            "Epoch [187/1000], Loss: 772533.2877\n",
            "Epoch [188/1000], Loss: 766532.8436\n",
            "Epoch [189/1000], Loss: 761146.6964\n",
            "Epoch [190/1000], Loss: 755793.7681\n",
            "Epoch [191/1000], Loss: 750350.4564\n",
            "Epoch [192/1000], Loss: 745227.1602\n",
            "Epoch [193/1000], Loss: 739336.8341\n",
            "Epoch [194/1000], Loss: 897361.5982\n",
            "Epoch [195/1000], Loss: 727509.6958\n",
            "Epoch [196/1000], Loss: 722171.4059\n",
            "Epoch [197/1000], Loss: 716250.4952\n",
            "Epoch [198/1000], Loss: 710823.4901\n",
            "Epoch [199/1000], Loss: 705527.9532\n",
            "Epoch [200/1000], Loss: 699738.5225\n",
            "Epoch [201/1000], Loss: 694390.8862\n",
            "Epoch [202/1000], Loss: 688684.7832\n",
            "Epoch [203/1000], Loss: 682989.6440\n",
            "Epoch [204/1000], Loss: 677428.4650\n",
            "Epoch [205/1000], Loss: 671960.1156\n",
            "Epoch [206/1000], Loss: 666157.5137\n",
            "Epoch [207/1000], Loss: 662309.7635\n",
            "Epoch [208/1000], Loss: 654920.0943\n",
            "Epoch [209/1000], Loss: 649113.9822\n",
            "Epoch [210/1000], Loss: 643539.2324\n",
            "Epoch [211/1000], Loss: 638036.3093\n",
            "Epoch [212/1000], Loss: 632008.2556\n",
            "Epoch [213/1000], Loss: 626324.8023\n",
            "Epoch [214/1000], Loss: 622015.9720\n",
            "Epoch [215/1000], Loss: 614752.3923\n",
            "Epoch [216/1000], Loss: 609496.1822\n",
            "Epoch [217/1000], Loss: 603211.8149\n",
            "Epoch [218/1000], Loss: 597905.2372\n",
            "Epoch [219/1000], Loss: 591611.2049\n",
            "Epoch [220/1000], Loss: 585994.4021\n",
            "Epoch [221/1000], Loss: 580193.2997\n",
            "Epoch [222/1000], Loss: 574570.2847\n",
            "Epoch [223/1000], Loss: 568524.7890\n",
            "Epoch [224/1000], Loss: 562786.3794\n",
            "Epoch [225/1000], Loss: 556890.4860\n",
            "Epoch [226/1000], Loss: 551349.8436\n",
            "Epoch [227/1000], Loss: 545445.5297\n",
            "Epoch [228/1000], Loss: 539724.6652\n",
            "Epoch [229/1000], Loss: 533757.7254\n",
            "Epoch [230/1000], Loss: 527825.2348\n",
            "Epoch [231/1000], Loss: 522322.6968\n",
            "Epoch [232/1000], Loss: 516264.5553\n",
            "Epoch [233/1000], Loss: 510494.8724\n",
            "Epoch [234/1000], Loss: 504771.3935\n",
            "Epoch [235/1000], Loss: 499270.6779\n",
            "Epoch [236/1000], Loss: 493230.5966\n",
            "Epoch [237/1000], Loss: 487286.0175\n",
            "Epoch [238/1000], Loss: 481980.3505\n",
            "Epoch [239/1000], Loss: 476035.5469\n",
            "Epoch [240/1000], Loss: 470327.2434\n",
            "Epoch [241/1000], Loss: 465129.1928\n",
            "Epoch [242/1000], Loss: 458614.0728\n",
            "Epoch [243/1000], Loss: 453252.0997\n",
            "Epoch [244/1000], Loss: 447077.1436\n",
            "Epoch [245/1000], Loss: 441286.5654\n",
            "Epoch [246/1000], Loss: 435626.1787\n",
            "Epoch [247/1000], Loss: 430448.4897\n",
            "Epoch [248/1000], Loss: 424217.1959\n",
            "Epoch [249/1000], Loss: 418756.2125\n",
            "Epoch [250/1000], Loss: 413093.0083\n",
            "Epoch [251/1000], Loss: 407380.5841\n",
            "Epoch [252/1000], Loss: 402838.9368\n",
            "Epoch [253/1000], Loss: 396249.4982\n",
            "Epoch [254/1000], Loss: 390590.6607\n",
            "Epoch [255/1000], Loss: 385183.4374\n",
            "Epoch [256/1000], Loss: 379680.5693\n",
            "Epoch [257/1000], Loss: 374128.2428\n",
            "Epoch [258/1000], Loss: 368479.4745\n",
            "Epoch [259/1000], Loss: 363050.1204\n",
            "Epoch [260/1000], Loss: 357677.7853\n",
            "Epoch [261/1000], Loss: 352093.8995\n",
            "Epoch [262/1000], Loss: 347956.6540\n",
            "Epoch [263/1000], Loss: 341268.7012\n",
            "Epoch [264/1000], Loss: 335936.5392\n",
            "Epoch [265/1000], Loss: 330758.1408\n",
            "Epoch [266/1000], Loss: 325690.0424\n",
            "Epoch [267/1000], Loss: 320049.2192\n",
            "Epoch [268/1000], Loss: 314680.8741\n",
            "Epoch [269/1000], Loss: 309561.6815\n",
            "Epoch [270/1000], Loss: 304237.3653\n",
            "Epoch [271/1000], Loss: 298981.8399\n",
            "Epoch [272/1000], Loss: 294403.2467\n",
            "Epoch [273/1000], Loss: 288783.1147\n",
            "Epoch [274/1000], Loss: 283742.9284\n",
            "Epoch [275/1000], Loss: 278651.6536\n",
            "Epoch [276/1000], Loss: 273680.9442\n",
            "Epoch [277/1000], Loss: 268586.7137\n",
            "Epoch [278/1000], Loss: 264456.2070\n",
            "Epoch [279/1000], Loss: 258717.7480\n",
            "Epoch [280/1000], Loss: 253888.1573\n",
            "Epoch [281/1000], Loss: 248997.5709\n",
            "Epoch [282/1000], Loss: 244181.7832\n",
            "Epoch [283/1000], Loss: 239418.9589\n",
            "Epoch [284/1000], Loss: 234679.0697\n",
            "Epoch [285/1000], Loss: 229889.4139\n",
            "Epoch [286/1000], Loss: 225231.8931\n",
            "Epoch [287/1000], Loss: 220781.1553\n",
            "Epoch [288/1000], Loss: 216013.4238\n",
            "Epoch [289/1000], Loss: 211467.7017\n",
            "Epoch [290/1000], Loss: 207033.1388\n",
            "Epoch [291/1000], Loss: 202498.1796\n",
            "Epoch [292/1000], Loss: 198076.4724\n",
            "Epoch [293/1000], Loss: 193736.4151\n",
            "Epoch [294/1000], Loss: 189410.6997\n",
            "Epoch [295/1000], Loss: 185122.9665\n",
            "Epoch [296/1000], Loss: 180874.4006\n",
            "Epoch [297/1000], Loss: 176690.7097\n",
            "Epoch [298/1000], Loss: 172610.6963\n",
            "Epoch [299/1000], Loss: 168385.7609\n",
            "Epoch [300/1000], Loss: 164298.6735\n",
            "Epoch [301/1000], Loss: 160296.5787\n",
            "Epoch [302/1000], Loss: 156502.4549\n",
            "Epoch [303/1000], Loss: 152421.0206\n",
            "Epoch [304/1000], Loss: 148493.9850\n",
            "Epoch [305/1000], Loss: 144667.0821\n",
            "Epoch [306/1000], Loss: 140971.6045\n",
            "Epoch [307/1000], Loss: 137129.1492\n",
            "Epoch [308/1000], Loss: 133406.6788\n",
            "Epoch [309/1000], Loss: 129764.9869\n",
            "Epoch [310/1000], Loss: 126749.4761\n",
            "Epoch [311/1000], Loss: 122650.9147\n",
            "Epoch [312/1000], Loss: 119161.1726\n",
            "Epoch [313/1000], Loss: 116282.0950\n",
            "Epoch [314/1000], Loss: 112352.3588\n",
            "Epoch [315/1000], Loss: 109213.9866\n",
            "Epoch [316/1000], Loss: 105781.4206\n",
            "Epoch [317/1000], Loss: 102560.8803\n",
            "Epoch [318/1000], Loss: 99370.7197\n",
            "Epoch [319/1000], Loss: 96243.9788\n",
            "Epoch [320/1000], Loss: 93224.3865\n",
            "Epoch [321/1000], Loss: 90312.3347\n",
            "Epoch [322/1000], Loss: 87199.8665\n",
            "Epoch [323/1000], Loss: 84281.4677\n",
            "Epoch [324/1000], Loss: 81420.0996\n",
            "Epoch [325/1000], Loss: 79300.9202\n",
            "Epoch [326/1000], Loss: 75856.9264\n",
            "Epoch [327/1000], Loss: 73181.4454\n",
            "Epoch [328/1000], Loss: 70511.1662\n",
            "Epoch [329/1000], Loss: 67957.2925\n",
            "Epoch [330/1000], Loss: 65512.4247\n",
            "Epoch [331/1000], Loss: 62941.9019\n",
            "Epoch [332/1000], Loss: 60525.4507\n",
            "Epoch [333/1000], Loss: 58392.5236\n",
            "Epoch [334/1000], Loss: 55909.9393\n",
            "Epoch [335/1000], Loss: 53666.0747\n",
            "Epoch [336/1000], Loss: 51445.8445\n",
            "Epoch [337/1000], Loss: 49283.4414\n",
            "Epoch [338/1000], Loss: 47376.4862\n",
            "Epoch [339/1000], Loss: 45148.3668\n",
            "Epoch [340/1000], Loss: 43551.7408\n",
            "Epoch [341/1000], Loss: 41257.8188\n",
            "Epoch [342/1000], Loss: 39413.2884\n",
            "Epoch [343/1000], Loss: 37589.1260\n",
            "Epoch [344/1000], Loss: 35820.9971\n",
            "Epoch [345/1000], Loss: 34123.9748\n",
            "Epoch [346/1000], Loss: 32457.7915\n",
            "Epoch [347/1000], Loss: 30883.0487\n",
            "Epoch [348/1000], Loss: 29308.9538\n",
            "Epoch [349/1000], Loss: 28216.1421\n",
            "Epoch [350/1000], Loss: 26367.9349\n",
            "Epoch [351/1000], Loss: 25009.3462\n",
            "Epoch [352/1000], Loss: 23696.9850\n",
            "Epoch [353/1000], Loss: 22333.0883\n",
            "Epoch [354/1000], Loss: 21115.4433\n",
            "Epoch [355/1000], Loss: 19933.7973\n",
            "Epoch [356/1000], Loss: 18799.2169\n",
            "Epoch [357/1000], Loss: 17644.4797\n",
            "Epoch [358/1000], Loss: 16614.2730\n",
            "Epoch [359/1000], Loss: 15558.0960\n",
            "Epoch [360/1000], Loss: 14898.0443\n",
            "Epoch [361/1000], Loss: 13683.9544\n",
            "Epoch [362/1000], Loss: 12778.0852\n",
            "Epoch [363/1000], Loss: 11979.2373\n",
            "Epoch [364/1000], Loss: 11157.2736\n",
            "Epoch [365/1000], Loss: 10793.1086\n",
            "Epoch [366/1000], Loss: 9649.7506\n",
            "Epoch [367/1000], Loss: 10967.3739\n",
            "Epoch [368/1000], Loss: 8218.4035\n",
            "Epoch [369/1000], Loss: 7620.7203\n",
            "Epoch [370/1000], Loss: 7047.9085\n",
            "Epoch [371/1000], Loss: 6493.6783\n",
            "Epoch [372/1000], Loss: 6006.0634\n",
            "Epoch [373/1000], Loss: 5513.0099\n",
            "Epoch [374/1000], Loss: 5142.2263\n",
            "Epoch [375/1000], Loss: 4706.0127\n",
            "Epoch [376/1000], Loss: 4292.3892\n",
            "Epoch [377/1000], Loss: 3950.8374\n",
            "Epoch [378/1000], Loss: 3652.1940\n",
            "Epoch [379/1000], Loss: 3322.3989\n",
            "Epoch [380/1000], Loss: 3000.4463\n",
            "Epoch [381/1000], Loss: 2797.0806\n",
            "Epoch [382/1000], Loss: 2511.5266\n",
            "Epoch [383/1000], Loss: 2302.1202\n",
            "Epoch [384/1000], Loss: 2073.4070\n",
            "Epoch [385/1000], Loss: 1893.6476\n",
            "Epoch [386/1000], Loss: 1719.9460\n",
            "Epoch [387/1000], Loss: 1545.3316\n",
            "Epoch [388/1000], Loss: 1441.3859\n",
            "Epoch [389/1000], Loss: 1285.1861\n",
            "Epoch [390/1000], Loss: 1167.0229\n",
            "Epoch [391/1000], Loss: 1052.5072\n",
            "Epoch [392/1000], Loss: 979.3482\n",
            "Epoch [393/1000], Loss: 860.1879\n",
            "Epoch [394/1000], Loss: 797.6495\n",
            "Epoch [395/1000], Loss: 711.7983\n",
            "Epoch [396/1000], Loss: 653.8053\n",
            "Epoch [397/1000], Loss: 614.7657\n",
            "Epoch [398/1000], Loss: 567.7782\n",
            "Epoch [399/1000], Loss: 507.6267\n",
            "Epoch [400/1000], Loss: 460.7966\n",
            "Epoch [401/1000], Loss: 432.1163\n",
            "Epoch [402/1000], Loss: 395.7402\n",
            "Epoch [403/1000], Loss: 367.3793\n",
            "Epoch [404/1000], Loss: 339.2074\n",
            "Epoch [405/1000], Loss: 305.7281\n",
            "Epoch [406/1000], Loss: 288.5790\n",
            "Epoch [407/1000], Loss: 267.7049\n",
            "Epoch [408/1000], Loss: 259.7863\n",
            "Epoch [409/1000], Loss: 243.7664\n",
            "Epoch [410/1000], Loss: 229.6667\n",
            "Epoch [411/1000], Loss: 228.1894\n",
            "Epoch [412/1000], Loss: 201.3368\n",
            "Epoch [413/1000], Loss: 194.0951\n",
            "Epoch [414/1000], Loss: 198.8438\n",
            "Epoch [415/1000], Loss: 198.0372\n",
            "Epoch [416/1000], Loss: 172.7478\n",
            "Epoch [417/1000], Loss: 167.8720\n",
            "Epoch [418/1000], Loss: 160.9104\n",
            "Epoch [419/1000], Loss: 155.9866\n",
            "Epoch [420/1000], Loss: 148.3980\n",
            "Epoch [421/1000], Loss: 153.5565\n",
            "Epoch [422/1000], Loss: 139.0793\n",
            "Epoch [423/1000], Loss: 138.1807\n",
            "Epoch [424/1000], Loss: 145.3488\n",
            "Epoch [425/1000], Loss: 137.7538\n",
            "Epoch [426/1000], Loss: 126.3106\n",
            "Epoch [427/1000], Loss: 118.6570\n",
            "Epoch [428/1000], Loss: 126.1760\n",
            "Epoch [429/1000], Loss: 110.8504\n",
            "Epoch [430/1000], Loss: 117.5540\n",
            "Epoch [431/1000], Loss: 114.6252\n",
            "Epoch [432/1000], Loss: 118.6145\n",
            "Epoch [433/1000], Loss: 110.2827\n",
            "Epoch [434/1000], Loss: 120.3893\n",
            "Epoch [435/1000], Loss: 115.5271\n",
            "Epoch [436/1000], Loss: 111.2335\n",
            "Epoch [437/1000], Loss: 119.9591\n",
            "Epoch [438/1000], Loss: 116.1233\n",
            "Epoch [439/1000], Loss: 104.0682\n",
            "Epoch [440/1000], Loss: 117.7270\n",
            "Epoch [441/1000], Loss: 98.6278\n",
            "Epoch [442/1000], Loss: 100.7540\n",
            "Epoch [443/1000], Loss: 98.4043\n",
            "Epoch [444/1000], Loss: 97.0983\n",
            "Epoch [445/1000], Loss: 112.1377\n",
            "Epoch [446/1000], Loss: 92.5959\n",
            "Epoch [447/1000], Loss: 88.4801\n",
            "Epoch [448/1000], Loss: 97.7167\n",
            "Epoch [449/1000], Loss: 97.1268\n",
            "Epoch [450/1000], Loss: 100.5540\n",
            "Epoch [451/1000], Loss: 91.5682\n",
            "Epoch [452/1000], Loss: 94.8868\n",
            "Epoch [453/1000], Loss: 94.4795\n",
            "Epoch [454/1000], Loss: 98.4296\n",
            "Epoch [455/1000], Loss: 91.7002\n",
            "Epoch [456/1000], Loss: 85.7453\n",
            "Epoch [457/1000], Loss: 88.1531\n",
            "Epoch [458/1000], Loss: 88.8385\n",
            "Epoch [459/1000], Loss: 90.8528\n",
            "Epoch [460/1000], Loss: 90.8099\n",
            "Epoch [461/1000], Loss: 81.4486\n",
            "Epoch [462/1000], Loss: 84.8878\n",
            "Epoch [463/1000], Loss: 86.2496\n",
            "Epoch [464/1000], Loss: 86.8637\n",
            "Epoch [465/1000], Loss: 79.6321\n",
            "Epoch [466/1000], Loss: 88.5478\n",
            "Epoch [467/1000], Loss: 80.2218\n",
            "Epoch [468/1000], Loss: 86.5368\n",
            "Epoch [469/1000], Loss: 75.6259\n",
            "Epoch [470/1000], Loss: 76.7803\n",
            "Epoch [471/1000], Loss: 79.8264\n",
            "Epoch [472/1000], Loss: 79.4846\n",
            "Epoch [473/1000], Loss: 79.0353\n",
            "Epoch [474/1000], Loss: 80.0653\n",
            "Epoch [475/1000], Loss: 77.0134\n",
            "Epoch [476/1000], Loss: 78.4680\n",
            "Epoch [477/1000], Loss: 83.8722\n",
            "Epoch [478/1000], Loss: 76.0598\n",
            "Epoch [479/1000], Loss: 83.9935\n",
            "Epoch [480/1000], Loss: 83.0108\n",
            "Epoch [481/1000], Loss: 81.8503\n",
            "Epoch [482/1000], Loss: 75.9488\n",
            "Epoch [483/1000], Loss: 87.0346\n",
            "Epoch [484/1000], Loss: 77.6344\n",
            "Epoch [485/1000], Loss: 91.0726\n",
            "Epoch [486/1000], Loss: 87.0009\n",
            "Epoch [487/1000], Loss: 79.0337\n",
            "Epoch [488/1000], Loss: 77.3592\n",
            "Epoch [489/1000], Loss: 75.4625\n",
            "Epoch [490/1000], Loss: 77.1867\n",
            "Epoch [491/1000], Loss: 84.2261\n",
            "Epoch [492/1000], Loss: 73.7580\n",
            "Epoch [493/1000], Loss: 83.5284\n",
            "Epoch [494/1000], Loss: 72.8835\n",
            "Epoch [495/1000], Loss: 88.6070\n",
            "Epoch [496/1000], Loss: 81.1641\n",
            "Epoch [497/1000], Loss: 86.0428\n",
            "Epoch [498/1000], Loss: 71.0712\n",
            "Epoch [499/1000], Loss: 83.0523\n",
            "Epoch [500/1000], Loss: 76.5042\n",
            "Epoch [501/1000], Loss: 75.3883\n",
            "Epoch [502/1000], Loss: 71.9754\n",
            "Epoch [503/1000], Loss: 77.1570\n",
            "Epoch [504/1000], Loss: 79.9627\n",
            "Epoch [505/1000], Loss: 71.1312\n",
            "Epoch [506/1000], Loss: 73.0261\n",
            "Epoch [507/1000], Loss: 74.0033\n",
            "Epoch [508/1000], Loss: 78.1097\n",
            "Epoch [509/1000], Loss: 87.1059\n",
            "Epoch [510/1000], Loss: 73.3494\n",
            "Epoch [511/1000], Loss: 69.1990\n",
            "Epoch [512/1000], Loss: 66.7749\n",
            "Epoch [513/1000], Loss: 75.3170\n",
            "Epoch [514/1000], Loss: 73.1613\n",
            "Epoch [515/1000], Loss: 74.7799\n",
            "Epoch [516/1000], Loss: 66.9974\n",
            "Epoch [517/1000], Loss: 73.4064\n",
            "Epoch [518/1000], Loss: 74.3840\n",
            "Epoch [519/1000], Loss: 83.1016\n",
            "Epoch [520/1000], Loss: 73.1268\n",
            "Epoch [521/1000], Loss: 73.9405\n",
            "Epoch [522/1000], Loss: 71.0570\n",
            "Epoch [523/1000], Loss: 69.1095\n",
            "Epoch [524/1000], Loss: 68.1005\n",
            "Epoch [525/1000], Loss: 74.8832\n",
            "Epoch [526/1000], Loss: 85.3748\n",
            "Epoch [527/1000], Loss: 68.4463\n",
            "Epoch [528/1000], Loss: 74.7791\n",
            "Epoch [529/1000], Loss: 72.7999\n",
            "Epoch [530/1000], Loss: 70.5468\n",
            "Epoch [531/1000], Loss: 68.1099\n",
            "Epoch [532/1000], Loss: 68.3849\n",
            "Epoch [533/1000], Loss: 71.2181\n",
            "Epoch [534/1000], Loss: 66.5136\n",
            "Epoch [535/1000], Loss: 70.3737\n",
            "Epoch [536/1000], Loss: 67.0273\n",
            "Epoch [537/1000], Loss: 69.4490\n",
            "Epoch [538/1000], Loss: 65.0489\n",
            "Epoch [539/1000], Loss: 66.4556\n",
            "Epoch [540/1000], Loss: 68.9101\n",
            "Epoch [541/1000], Loss: 74.2339\n",
            "Epoch [542/1000], Loss: 65.5024\n",
            "Epoch [543/1000], Loss: 66.7000\n",
            "Epoch [544/1000], Loss: 78.7476\n",
            "Epoch [545/1000], Loss: 79.4982\n",
            "Epoch [546/1000], Loss: 64.6063\n",
            "Epoch [547/1000], Loss: 76.2594\n",
            "Epoch [548/1000], Loss: 84.1914\n",
            "Epoch [549/1000], Loss: 65.6423\n",
            "Epoch [550/1000], Loss: 64.0593\n",
            "Epoch [551/1000], Loss: 69.0303\n",
            "Epoch [552/1000], Loss: 76.5303\n",
            "Epoch [553/1000], Loss: 78.4610\n",
            "Epoch [554/1000], Loss: 67.8649\n",
            "Epoch [555/1000], Loss: 82.1384\n",
            "Epoch [556/1000], Loss: 74.0740\n",
            "Epoch [557/1000], Loss: 64.6518\n",
            "Epoch [558/1000], Loss: 73.4601\n",
            "Epoch [559/1000], Loss: 65.4851\n",
            "Epoch [560/1000], Loss: 66.9835\n",
            "Epoch [561/1000], Loss: 74.9717\n",
            "Epoch [562/1000], Loss: 66.1243\n",
            "Epoch [563/1000], Loss: 69.2101\n",
            "Epoch [564/1000], Loss: 73.5185\n",
            "Epoch [565/1000], Loss: 70.2174\n",
            "Epoch [566/1000], Loss: 71.8626\n",
            "Epoch [567/1000], Loss: 65.4330\n",
            "Epoch [568/1000], Loss: 62.9586\n",
            "Epoch [569/1000], Loss: 68.3037\n",
            "Epoch [570/1000], Loss: 62.9368\n",
            "Epoch [571/1000], Loss: 70.6619\n",
            "Epoch [572/1000], Loss: 71.6746\n",
            "Epoch [573/1000], Loss: 70.3053\n",
            "Epoch [574/1000], Loss: 80.9421\n",
            "Epoch [575/1000], Loss: 67.5575\n",
            "Epoch [576/1000], Loss: 67.9780\n",
            "Epoch [577/1000], Loss: 63.0754\n",
            "Epoch [578/1000], Loss: 78.6762\n",
            "Epoch [579/1000], Loss: 81.8869\n",
            "Epoch [580/1000], Loss: 66.3819\n",
            "Epoch [581/1000], Loss: 73.5587\n",
            "Epoch [582/1000], Loss: 62.2914\n",
            "Epoch [583/1000], Loss: 65.4745\n",
            "Epoch [584/1000], Loss: 67.6213\n",
            "Epoch [585/1000], Loss: 58.1636\n",
            "Epoch [586/1000], Loss: 67.0034\n",
            "Epoch [587/1000], Loss: 67.7513\n",
            "Epoch [588/1000], Loss: 64.2620\n",
            "Epoch [589/1000], Loss: 67.3232\n",
            "Epoch [590/1000], Loss: 77.4259\n",
            "Epoch [591/1000], Loss: 63.4511\n",
            "Epoch [592/1000], Loss: 67.6244\n",
            "Epoch [593/1000], Loss: 65.1438\n",
            "Epoch [594/1000], Loss: 69.1660\n",
            "Epoch [595/1000], Loss: 63.8220\n",
            "Epoch [596/1000], Loss: 64.6900\n",
            "Epoch [597/1000], Loss: 60.1629\n",
            "Epoch [598/1000], Loss: 66.3045\n",
            "Epoch [599/1000], Loss: 61.7411\n",
            "Epoch [600/1000], Loss: 63.4957\n",
            "Epoch [601/1000], Loss: 71.9660\n",
            "Epoch [602/1000], Loss: 70.8034\n",
            "Epoch [603/1000], Loss: 63.3966\n",
            "Epoch [604/1000], Loss: 72.3952\n",
            "Epoch [605/1000], Loss: 63.3396\n",
            "Epoch [606/1000], Loss: 58.9778\n",
            "Epoch [607/1000], Loss: 72.1963\n",
            "Epoch [608/1000], Loss: 60.5187\n",
            "Epoch [609/1000], Loss: 71.0850\n",
            "Epoch [610/1000], Loss: 67.4097\n",
            "Epoch [611/1000], Loss: 69.1648\n",
            "Epoch [612/1000], Loss: 63.1301\n",
            "Epoch [613/1000], Loss: 60.7420\n",
            "Epoch [614/1000], Loss: 72.3893\n",
            "Epoch [615/1000], Loss: 81.4761\n",
            "Epoch [616/1000], Loss: 70.4863\n",
            "Epoch [617/1000], Loss: 65.6564\n",
            "Epoch [618/1000], Loss: 67.1148\n",
            "Epoch [619/1000], Loss: 66.1896\n",
            "Epoch [620/1000], Loss: 67.6272\n",
            "Epoch [621/1000], Loss: 62.2702\n",
            "Epoch [622/1000], Loss: 72.9458\n",
            "Epoch [623/1000], Loss: 62.4307\n",
            "Epoch [624/1000], Loss: 62.5118\n",
            "Epoch [625/1000], Loss: 72.3459\n",
            "Epoch [626/1000], Loss: 60.6057\n",
            "Epoch [627/1000], Loss: 64.5072\n",
            "Epoch [628/1000], Loss: 61.9799\n",
            "Epoch [629/1000], Loss: 58.7825\n",
            "Epoch [630/1000], Loss: 64.2292\n",
            "Epoch [631/1000], Loss: 64.0454\n",
            "Epoch [632/1000], Loss: 62.0949\n",
            "Epoch [633/1000], Loss: 64.0437\n",
            "Epoch [634/1000], Loss: 60.3151\n",
            "Epoch [635/1000], Loss: 66.3598\n",
            "Epoch [636/1000], Loss: 94.3621\n",
            "Epoch [637/1000], Loss: 70.7214\n",
            "Epoch [638/1000], Loss: 65.7997\n",
            "Epoch [639/1000], Loss: 63.4476\n",
            "Epoch [640/1000], Loss: 64.3839\n",
            "Epoch [641/1000], Loss: 69.6089\n",
            "Epoch [642/1000], Loss: 57.3681\n",
            "Epoch [643/1000], Loss: 60.6917\n",
            "Epoch [644/1000], Loss: 62.7049\n",
            "Epoch [645/1000], Loss: 64.9734\n",
            "Epoch [646/1000], Loss: 67.8682\n",
            "Epoch [647/1000], Loss: 60.2578\n",
            "Epoch [648/1000], Loss: 63.5127\n",
            "Epoch [649/1000], Loss: 58.3891\n",
            "Epoch [650/1000], Loss: 57.7515\n",
            "Epoch [651/1000], Loss: 54.4693\n",
            "Epoch [652/1000], Loss: 59.7321\n",
            "Epoch [653/1000], Loss: 72.3270\n",
            "Epoch [654/1000], Loss: 69.7568\n",
            "Epoch [655/1000], Loss: 75.3079\n",
            "Epoch [656/1000], Loss: 66.1876\n",
            "Epoch [657/1000], Loss: 61.3485\n",
            "Epoch [658/1000], Loss: 58.8872\n",
            "Epoch [659/1000], Loss: 62.0954\n",
            "Epoch [660/1000], Loss: 63.0268\n",
            "Epoch [661/1000], Loss: 61.6152\n",
            "Epoch [662/1000], Loss: 58.9581\n",
            "Epoch [663/1000], Loss: 81.7632\n",
            "Epoch [664/1000], Loss: 62.6326\n",
            "Epoch [665/1000], Loss: 70.2129\n",
            "Epoch [666/1000], Loss: 67.6179\n",
            "Epoch [667/1000], Loss: 80.9681\n",
            "Epoch [668/1000], Loss: 63.2038\n",
            "Epoch [669/1000], Loss: 81.5434\n",
            "Epoch [670/1000], Loss: 63.4402\n",
            "Epoch [671/1000], Loss: 58.3705\n",
            "Epoch [672/1000], Loss: 69.7125\n",
            "Epoch [673/1000], Loss: 56.7143\n",
            "Epoch [674/1000], Loss: 64.8611\n",
            "Epoch [675/1000], Loss: 63.0241\n",
            "Epoch [676/1000], Loss: 60.2911\n",
            "Epoch [677/1000], Loss: 66.0666\n",
            "Epoch [678/1000], Loss: 62.7928\n",
            "Epoch [679/1000], Loss: 56.8668\n",
            "Epoch [680/1000], Loss: 62.1837\n",
            "Epoch [681/1000], Loss: 69.4910\n",
            "Epoch [682/1000], Loss: 58.7620\n",
            "Epoch [683/1000], Loss: 61.1416\n",
            "Epoch [684/1000], Loss: 61.8441\n",
            "Epoch [685/1000], Loss: 69.9207\n",
            "Epoch [686/1000], Loss: 73.5406\n",
            "Epoch [687/1000], Loss: 69.9866\n",
            "Epoch [688/1000], Loss: 82.3482\n",
            "Epoch [689/1000], Loss: 66.9677\n",
            "Epoch [690/1000], Loss: 61.5333\n",
            "Epoch [691/1000], Loss: 57.4627\n",
            "Epoch [692/1000], Loss: 55.4980\n",
            "Epoch [693/1000], Loss: 70.4678\n",
            "Epoch [694/1000], Loss: 62.6293\n",
            "Epoch [695/1000], Loss: 61.4524\n",
            "Epoch [696/1000], Loss: 69.4826\n",
            "Epoch [697/1000], Loss: 72.3304\n",
            "Epoch [698/1000], Loss: 68.1363\n",
            "Epoch [699/1000], Loss: 112.3439\n",
            "Epoch [700/1000], Loss: 62.2118\n",
            "Epoch [701/1000], Loss: 63.5759\n",
            "Epoch [702/1000], Loss: 56.9659\n",
            "Epoch [703/1000], Loss: 60.7355\n",
            "Epoch [704/1000], Loss: 58.8000\n",
            "Epoch [705/1000], Loss: 69.4118\n",
            "Epoch [706/1000], Loss: 70.5911\n",
            "Epoch [707/1000], Loss: 63.8028\n",
            "Epoch [708/1000], Loss: 59.8785\n",
            "Epoch [709/1000], Loss: 63.4763\n",
            "Epoch [710/1000], Loss: 58.0599\n",
            "Epoch [711/1000], Loss: 73.2727\n",
            "Epoch [712/1000], Loss: 56.4891\n",
            "Epoch [713/1000], Loss: 64.9671\n",
            "Epoch [714/1000], Loss: 58.8473\n",
            "Epoch [715/1000], Loss: 63.5864\n",
            "Epoch [716/1000], Loss: 57.9574\n",
            "Epoch [717/1000], Loss: 56.4277\n",
            "Epoch [718/1000], Loss: 60.2097\n",
            "Epoch [719/1000], Loss: 58.1187\n",
            "Epoch [720/1000], Loss: 69.3941\n",
            "Epoch [721/1000], Loss: 65.1757\n",
            "Epoch [722/1000], Loss: 63.8452\n",
            "Epoch [723/1000], Loss: 57.1498\n",
            "Epoch [724/1000], Loss: 58.9637\n",
            "Epoch [725/1000], Loss: 56.5129\n",
            "Epoch [726/1000], Loss: 62.2360\n",
            "Epoch [727/1000], Loss: 63.4131\n",
            "Epoch [728/1000], Loss: 64.6651\n",
            "Epoch [729/1000], Loss: 97.5782\n",
            "Epoch [730/1000], Loss: 65.2848\n",
            "Epoch [731/1000], Loss: 60.9535\n",
            "Epoch [732/1000], Loss: 59.5292\n",
            "Epoch [733/1000], Loss: 58.0531\n",
            "Epoch [734/1000], Loss: 57.1129\n",
            "Epoch [735/1000], Loss: 60.4024\n",
            "Epoch [736/1000], Loss: 67.3624\n",
            "Epoch [737/1000], Loss: 59.3000\n",
            "Epoch [738/1000], Loss: 63.0660\n",
            "Epoch [739/1000], Loss: 67.1704\n",
            "Epoch [740/1000], Loss: 65.6975\n",
            "Epoch [741/1000], Loss: 62.5655\n",
            "Epoch [742/1000], Loss: 70.3179\n",
            "Epoch [743/1000], Loss: 59.1209\n",
            "Epoch [744/1000], Loss: 64.1070\n",
            "Epoch [745/1000], Loss: 57.5256\n",
            "Epoch [746/1000], Loss: 66.2838\n",
            "Epoch [747/1000], Loss: 58.3771\n",
            "Epoch [748/1000], Loss: 70.3756\n",
            "Epoch [749/1000], Loss: 58.2463\n",
            "Epoch [750/1000], Loss: 58.9403\n",
            "Epoch [751/1000], Loss: 58.7768\n",
            "Epoch [752/1000], Loss: 59.3902\n",
            "Epoch [753/1000], Loss: 57.7198\n",
            "Epoch [754/1000], Loss: 57.9479\n",
            "Epoch [755/1000], Loss: 61.5523\n",
            "Epoch [756/1000], Loss: 66.0639\n",
            "Epoch [757/1000], Loss: 55.1801\n",
            "Epoch [758/1000], Loss: 64.1137\n",
            "Epoch [759/1000], Loss: 65.8742\n",
            "Epoch [760/1000], Loss: 61.1550\n",
            "Epoch [761/1000], Loss: 63.6460\n",
            "Epoch [762/1000], Loss: 57.1030\n",
            "Epoch [763/1000], Loss: 67.7703\n",
            "Epoch [764/1000], Loss: 65.9963\n",
            "Epoch [765/1000], Loss: 60.1127\n",
            "Epoch [766/1000], Loss: 67.0974\n",
            "Epoch [767/1000], Loss: 63.8853\n",
            "Epoch [768/1000], Loss: 63.3914\n",
            "Epoch [769/1000], Loss: 67.1990\n",
            "Epoch [770/1000], Loss: 55.9119\n",
            "Epoch [771/1000], Loss: 53.3952\n",
            "Epoch [772/1000], Loss: 58.7687\n",
            "Epoch [773/1000], Loss: 53.8407\n",
            "Epoch [774/1000], Loss: 56.9134\n",
            "Epoch [775/1000], Loss: 57.7260\n",
            "Epoch [776/1000], Loss: 63.0702\n",
            "Epoch [777/1000], Loss: 90.2343\n",
            "Epoch [778/1000], Loss: 61.0174\n",
            "Epoch [779/1000], Loss: 55.3145\n",
            "Epoch [780/1000], Loss: 65.5578\n",
            "Epoch [781/1000], Loss: 71.7578\n",
            "Epoch [782/1000], Loss: 63.8802\n",
            "Epoch [783/1000], Loss: 59.0156\n",
            "Epoch [784/1000], Loss: 60.5277\n",
            "Epoch [785/1000], Loss: 66.4781\n",
            "Epoch [786/1000], Loss: 57.2384\n",
            "Epoch [787/1000], Loss: 60.9538\n",
            "Epoch [788/1000], Loss: 62.6504\n",
            "Epoch [789/1000], Loss: 57.0454\n",
            "Epoch [790/1000], Loss: 80.5160\n",
            "Epoch [791/1000], Loss: 74.9949\n",
            "Epoch [792/1000], Loss: 59.7395\n",
            "Epoch [793/1000], Loss: 61.5906\n",
            "Epoch [794/1000], Loss: 55.2760\n",
            "Epoch [795/1000], Loss: 57.3914\n",
            "Epoch [796/1000], Loss: 55.9920\n",
            "Epoch [797/1000], Loss: 68.1364\n",
            "Epoch [798/1000], Loss: 63.6613\n",
            "Epoch [799/1000], Loss: 59.5668\n",
            "Epoch [800/1000], Loss: 65.1409\n",
            "Epoch [801/1000], Loss: 87.4645\n",
            "Epoch [802/1000], Loss: 87.3390\n",
            "Epoch [803/1000], Loss: 67.5196\n",
            "Epoch [804/1000], Loss: 65.2290\n",
            "Epoch [805/1000], Loss: 54.5291\n",
            "Epoch [806/1000], Loss: 66.2968\n",
            "Epoch [807/1000], Loss: 61.4704\n",
            "Epoch [808/1000], Loss: 70.8183\n",
            "Epoch [809/1000], Loss: 71.8925\n",
            "Epoch [810/1000], Loss: 59.9152\n",
            "Epoch [811/1000], Loss: 55.8816\n",
            "Epoch [812/1000], Loss: 54.4003\n",
            "Epoch [813/1000], Loss: 56.6580\n",
            "Epoch [814/1000], Loss: 58.6611\n",
            "Epoch [815/1000], Loss: 64.9124\n",
            "Epoch [816/1000], Loss: 60.9252\n",
            "Epoch [817/1000], Loss: 77.3005\n",
            "Epoch [818/1000], Loss: 65.5118\n",
            "Epoch [819/1000], Loss: 62.4225\n",
            "Epoch [820/1000], Loss: 65.1473\n",
            "Epoch [821/1000], Loss: 56.9397\n",
            "Epoch [822/1000], Loss: 61.9212\n",
            "Epoch [823/1000], Loss: 52.8619\n",
            "Epoch [824/1000], Loss: 56.9046\n",
            "Epoch [825/1000], Loss: 62.9467\n",
            "Epoch [826/1000], Loss: 57.4209\n",
            "Epoch [827/1000], Loss: 69.7897\n",
            "Epoch [828/1000], Loss: 57.1853\n",
            "Epoch [829/1000], Loss: 65.2395\n",
            "Epoch [830/1000], Loss: 57.1552\n",
            "Epoch [831/1000], Loss: 56.3603\n",
            "Epoch [832/1000], Loss: 53.3708\n",
            "Epoch [833/1000], Loss: 60.8682\n",
            "Epoch [834/1000], Loss: 73.6519\n",
            "Epoch [835/1000], Loss: 72.6073\n",
            "Epoch [836/1000], Loss: 72.5250\n",
            "Epoch [837/1000], Loss: 64.2626\n",
            "Epoch [838/1000], Loss: 64.7563\n",
            "Epoch [839/1000], Loss: 55.5459\n",
            "Epoch [840/1000], Loss: 54.4230\n",
            "Epoch [841/1000], Loss: 58.3814\n",
            "Epoch [842/1000], Loss: 54.8121\n",
            "Epoch [843/1000], Loss: 61.2247\n",
            "Epoch [844/1000], Loss: 62.3566\n",
            "Epoch [845/1000], Loss: 68.7656\n",
            "Epoch [846/1000], Loss: 61.1282\n",
            "Epoch [847/1000], Loss: 60.5079\n",
            "Epoch [848/1000], Loss: 56.6584\n",
            "Epoch [849/1000], Loss: 57.9872\n",
            "Epoch [850/1000], Loss: 58.2636\n",
            "Epoch [851/1000], Loss: 59.4306\n",
            "Epoch [852/1000], Loss: 58.8566\n",
            "Epoch [853/1000], Loss: 73.5994\n",
            "Epoch [854/1000], Loss: 83.8728\n",
            "Epoch [855/1000], Loss: 68.4229\n",
            "Epoch [856/1000], Loss: 69.8929\n",
            "Epoch [857/1000], Loss: 55.4201\n",
            "Epoch [858/1000], Loss: 71.9503\n",
            "Epoch [859/1000], Loss: 55.5132\n",
            "Epoch [860/1000], Loss: 57.9814\n",
            "Epoch [861/1000], Loss: 66.3685\n",
            "Epoch [862/1000], Loss: 75.7565\n",
            "Epoch [863/1000], Loss: 66.2126\n",
            "Epoch [864/1000], Loss: 59.4125\n",
            "Epoch [865/1000], Loss: 58.1203\n",
            "Epoch [866/1000], Loss: 57.7071\n",
            "Epoch [867/1000], Loss: 56.8173\n",
            "Epoch [868/1000], Loss: 63.7236\n",
            "Epoch [869/1000], Loss: 62.8184\n",
            "Epoch [870/1000], Loss: 59.0238\n",
            "Epoch [871/1000], Loss: 55.2252\n",
            "Epoch [872/1000], Loss: 75.9462\n",
            "Epoch [873/1000], Loss: 55.4914\n",
            "Epoch [874/1000], Loss: 60.9644\n",
            "Epoch [875/1000], Loss: 53.9643\n",
            "Epoch [876/1000], Loss: 55.0943\n",
            "Epoch [877/1000], Loss: 70.6910\n",
            "Epoch [878/1000], Loss: 58.4487\n",
            "Epoch [879/1000], Loss: 56.5203\n",
            "Epoch [880/1000], Loss: 65.2811\n",
            "Epoch [881/1000], Loss: 60.9387\n",
            "Epoch [882/1000], Loss: 70.9753\n",
            "Epoch [883/1000], Loss: 66.1474\n",
            "Epoch [884/1000], Loss: 69.9858\n",
            "Epoch [885/1000], Loss: 78.1205\n",
            "Epoch [886/1000], Loss: 61.6980\n",
            "Epoch [887/1000], Loss: 53.9899\n",
            "Epoch [888/1000], Loss: 52.9617\n",
            "Epoch [889/1000], Loss: 75.3020\n",
            "Epoch [890/1000], Loss: 60.8534\n",
            "Epoch [891/1000], Loss: 53.8985\n",
            "Epoch [892/1000], Loss: 65.2080\n",
            "Epoch [893/1000], Loss: 58.3303\n",
            "Epoch [894/1000], Loss: 66.1722\n",
            "Epoch [895/1000], Loss: 53.6339\n",
            "Epoch [896/1000], Loss: 53.1193\n",
            "Epoch [897/1000], Loss: 56.1138\n",
            "Epoch [898/1000], Loss: 56.4596\n",
            "Epoch [899/1000], Loss: 56.3071\n",
            "Epoch [900/1000], Loss: 65.7203\n",
            "Epoch [901/1000], Loss: 55.2284\n",
            "Epoch [902/1000], Loss: 53.4952\n",
            "Epoch [903/1000], Loss: 56.4427\n",
            "Epoch [904/1000], Loss: 58.7379\n",
            "Epoch [905/1000], Loss: 57.6517\n",
            "Epoch [906/1000], Loss: 56.0830\n",
            "Epoch [907/1000], Loss: 67.3978\n",
            "Epoch [908/1000], Loss: 86.9353\n",
            "Epoch [909/1000], Loss: 59.4992\n",
            "Epoch [910/1000], Loss: 59.3548\n",
            "Epoch [911/1000], Loss: 61.1422\n",
            "Epoch [912/1000], Loss: 55.9803\n",
            "Epoch [913/1000], Loss: 57.2052\n",
            "Epoch [914/1000], Loss: 57.4742\n",
            "Epoch [915/1000], Loss: 68.8017\n",
            "Epoch [916/1000], Loss: 52.0027\n",
            "Epoch [917/1000], Loss: 60.5786\n",
            "Epoch [918/1000], Loss: 53.6486\n",
            "Epoch [919/1000], Loss: 58.2866\n",
            "Epoch [920/1000], Loss: 63.5023\n",
            "Epoch [921/1000], Loss: 61.7980\n",
            "Epoch [922/1000], Loss: 65.6114\n",
            "Epoch [923/1000], Loss: 63.6569\n",
            "Epoch [924/1000], Loss: 60.8828\n",
            "Epoch [925/1000], Loss: 57.9276\n",
            "Epoch [926/1000], Loss: 55.8339\n",
            "Epoch [927/1000], Loss: 63.4739\n",
            "Epoch [928/1000], Loss: 61.3309\n",
            "Epoch [929/1000], Loss: 64.3791\n",
            "Epoch [930/1000], Loss: 54.9901\n",
            "Epoch [931/1000], Loss: 56.9453\n",
            "Epoch [932/1000], Loss: 72.9546\n",
            "Epoch [933/1000], Loss: 63.2676\n",
            "Epoch [934/1000], Loss: 57.2338\n",
            "Epoch [935/1000], Loss: 62.5639\n",
            "Epoch [936/1000], Loss: 64.7889\n",
            "Epoch [937/1000], Loss: 58.3205\n",
            "Epoch [938/1000], Loss: 68.5840\n",
            "Epoch [939/1000], Loss: 65.6909\n",
            "Epoch [940/1000], Loss: 61.8507\n",
            "Epoch [941/1000], Loss: 58.3633\n",
            "Epoch [942/1000], Loss: 58.1002\n",
            "Epoch [943/1000], Loss: 61.5051\n",
            "Epoch [944/1000], Loss: 65.3823\n",
            "Epoch [945/1000], Loss: 53.9390\n",
            "Epoch [946/1000], Loss: 60.9631\n",
            "Epoch [947/1000], Loss: 60.1883\n",
            "Epoch [948/1000], Loss: 81.5963\n",
            "Epoch [949/1000], Loss: 63.9035\n",
            "Epoch [950/1000], Loss: 62.8484\n",
            "Epoch [951/1000], Loss: 56.6444\n",
            "Epoch [952/1000], Loss: 66.0240\n",
            "Epoch [953/1000], Loss: 55.0508\n",
            "Epoch [954/1000], Loss: 85.5938\n",
            "Epoch [955/1000], Loss: 75.2302\n",
            "Epoch [956/1000], Loss: 64.0900\n",
            "Epoch [957/1000], Loss: 59.8908\n",
            "Epoch [958/1000], Loss: 57.0202\n",
            "Epoch [959/1000], Loss: 54.2315\n",
            "Epoch [960/1000], Loss: 60.0375\n",
            "Epoch [961/1000], Loss: 62.6854\n",
            "Epoch [962/1000], Loss: 69.7122\n",
            "Epoch [963/1000], Loss: 60.6789\n",
            "Epoch [964/1000], Loss: 69.6407\n",
            "Epoch [965/1000], Loss: 76.6994\n",
            "Epoch [966/1000], Loss: 62.6204\n",
            "Epoch [967/1000], Loss: 60.9965\n",
            "Epoch [968/1000], Loss: 58.4333\n",
            "Epoch [969/1000], Loss: 54.9602\n",
            "Epoch [970/1000], Loss: 53.8413\n",
            "Epoch [971/1000], Loss: 61.0154\n",
            "Epoch [972/1000], Loss: 57.5317\n",
            "Epoch [973/1000], Loss: 63.0932\n",
            "Epoch [974/1000], Loss: 67.3361\n",
            "Epoch [975/1000], Loss: 63.0562\n",
            "Epoch [976/1000], Loss: 61.9102\n",
            "Epoch [977/1000], Loss: 53.0641\n",
            "Epoch [978/1000], Loss: 53.3511\n",
            "Epoch [979/1000], Loss: 58.8621\n",
            "Epoch [980/1000], Loss: 56.9378\n",
            "Epoch [981/1000], Loss: 56.2537\n",
            "Epoch [982/1000], Loss: 64.0017\n",
            "Epoch [983/1000], Loss: 57.5106\n",
            "Epoch [984/1000], Loss: 57.9487\n",
            "Epoch [985/1000], Loss: 54.3415\n",
            "Epoch [986/1000], Loss: 55.3455\n",
            "Epoch [987/1000], Loss: 66.7781\n",
            "Epoch [988/1000], Loss: 61.6944\n",
            "Epoch [989/1000], Loss: 59.4325\n",
            "Epoch [990/1000], Loss: 72.9952\n",
            "Epoch [991/1000], Loss: 69.1218\n",
            "Epoch [992/1000], Loss: 78.2496\n",
            "Epoch [993/1000], Loss: 65.6210\n",
            "Epoch [994/1000], Loss: 61.8871\n",
            "Epoch [995/1000], Loss: 65.6046\n",
            "Epoch [996/1000], Loss: 89.2951\n",
            "Epoch [997/1000], Loss: 65.2829\n",
            "Epoch [998/1000], Loss: 65.9305\n",
            "Epoch [999/1000], Loss: 61.3425\n",
            "Epoch [1000/1000], Loss: 56.2049\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), 'pipe_model.pth') # Save model"
      ],
      "metadata": {
        "id": "HWYImDpaCjI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Stpe 5를 제외한 모든 step에 적용\n",
        "test_df = pd.read_excel('regression_step6.xlsx') # 예시 : step 6\n",
        "\n",
        "X_test = test_df[['new_input1', 'new_input2']].values\n",
        "y_test = test_df['작업 시간'].values\n",
        "\n",
        "scaler_test = StandardScaler()\n",
        "X_test = scaler_test.fit_transform(X_test)\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "uPo2x5ic7Oz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 모델 테스트 함수 (output 값 저장)\n",
        "def test(model, test_loader):\n",
        "    model.load_state_dict(torch.load('pipe_model.pth'))  # 모델 가중치 불러오기\n",
        "    model.eval()\n",
        "    outputs_list = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, targets in test_loader:\n",
        "            outputs = model(inputs.to(device))\n",
        "\n",
        "            # outputs 값을 리스트에 저장\n",
        "            outputs_list.extend(outputs.cpu().numpy())  # GPU에서 CPU로 변환 후 numpy로 변환하여 저장\n",
        "\n",
        "    return [round(output.item(), 1) for output in outputs_list]"
      ],
      "metadata": {
        "id": "3vn28-_HuQOy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = test(model, test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lXKKqgn02bMm",
        "outputId": "86bf3022-57e8-4687-bc59-5e3232cf030f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-55-549b2b57cac8>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load('pipe_model.pth'))  # 모델 가중치 불러오기\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['output'] = outputs"
      ],
      "metadata": {
        "id": "KVZXsciC2pmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "srWVbTIb_H7Y",
        "outputId": "13438bc4-89ea-4b97-99f8-34073362955d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-21.7,\n",
              " 103.6,\n",
              " 133.9,\n",
              " 56.8,\n",
              " 57.5,\n",
              " 56.4,\n",
              " 53.7,\n",
              " 0.1,\n",
              " 110.5,\n",
              " 45.4,\n",
              " -2.8,\n",
              " 32.7,\n",
              " 1.1,\n",
              " 95.1,\n",
              " 16.6,\n",
              " 4.4,\n",
              " 3.4,\n",
              " 6.6,\n",
              " 12.8,\n",
              " 4.2,\n",
              " 13.6,\n",
              " 2.7,\n",
              " 234.6,\n",
              " 9.5,\n",
              " 5.0,\n",
              " 8.5,\n",
              " 22.4,\n",
              " 2.5,\n",
              " 83.0,\n",
              " 44.3,\n",
              " 7.3,\n",
              " 6.5,\n",
              " 13.0,\n",
              " 6.1,\n",
              " -5.0,\n",
              " 15.8,\n",
              " 6.4,\n",
              " 12.3,\n",
              " 2.8,\n",
              " 161.8,\n",
              " 5.1,\n",
              " -1.8,\n",
              " -3.2,\n",
              " -9.5,\n",
              " -5.8,\n",
              " -10.3,\n",
              " 62.6,\n",
              " -3.3,\n",
              " 4.2,\n",
              " 1.9,\n",
              " 2.0,\n",
              " 16.8,\n",
              " -3.7,\n",
              " -2.3,\n",
              " 8.1,\n",
              " 6.9,\n",
              " 9.3,\n",
              " 24.2,\n",
              " 33.2,\n",
              " 19.1,\n",
              " 6.4,\n",
              " 4484.3,\n",
              " -4.4,\n",
              " -1.5,\n",
              " -1.8,\n",
              " 6.0,\n",
              " 7.8,\n",
              " 3.9,\n",
              " 11.6,\n",
              " -8.8,\n",
              " 60.0,\n",
              " 12.7,\n",
              " 2.2,\n",
              " -0.8,\n",
              " -6.8,\n",
              " -3.6,\n",
              " 2.3,\n",
              " 5.4,\n",
              " -12.0,\n",
              " 0.1,\n",
              " -5.0,\n",
              " -6.1,\n",
              " 54.1,\n",
              " -12.8,\n",
              " -21.2,\n",
              " -13.9,\n",
              " -12.0,\n",
              " 189.1,\n",
              " -14.5,\n",
              " -6.3,\n",
              " -9.7,\n",
              " -12.5,\n",
              " -14.0,\n",
              " -14.8,\n",
              " -13.3,\n",
              " -12.2,\n",
              " -6.7,\n",
              " 97.2,\n",
              " -13.0,\n",
              " -12.4,\n",
              " -14.7,\n",
              " -12.4,\n",
              " -11.9,\n",
              " -10.3,\n",
              " -22.2,\n",
              " 5.4,\n",
              " -12.1,\n",
              " -16.0,\n",
              " -13.0,\n",
              " -13.9,\n",
              " -3.8,\n",
              " -13.3,\n",
              " -13.8,\n",
              " -13.4,\n",
              " 26.9,\n",
              " -12.1,\n",
              " -13.0,\n",
              " -13.8,\n",
              " -11.6,\n",
              " 16.0,\n",
              " -10.7,\n",
              " -12.5,\n",
              " -14.1,\n",
              " -12.4,\n",
              " -8.8,\n",
              " -2.0,\n",
              " 25.6,\n",
              " -12.5,\n",
              " -13.2,\n",
              " -4.3,\n",
              " -12.3,\n",
              " -9.3,\n",
              " -5.9,\n",
              " -12.4,\n",
              " 19.6,\n",
              " -10.8,\n",
              " -4.9,\n",
              " -12.6,\n",
              " -11.2,\n",
              " -9.0,\n",
              " 50.8,\n",
              " -5.4,\n",
              " -8.0,\n",
              " -12.8,\n",
              " 51.6,\n",
              " -4.9,\n",
              " 8.4,\n",
              " -7.3,\n",
              " -20.0,\n",
              " -8.0,\n",
              " 1.6,\n",
              " -5.8,\n",
              " 229.5,\n",
              " 46.8,\n",
              " -4.9,\n",
              " -3.4,\n",
              " -0.2,\n",
              " 0.5,\n",
              " 94.5,\n",
              " -3.8,\n",
              " -3.4,\n",
              " 1.0,\n",
              " -4.2,\n",
              " -3.4,\n",
              " -3.9,\n",
              " -4.7,\n",
              " 15.8,\n",
              " -12.0,\n",
              " -12.2,\n",
              " -9.4,\n",
              " -11.5,\n",
              " -12.7,\n",
              " -12.6,\n",
              " -12.3,\n",
              " -7.6,\n",
              " -6.1,\n",
              " -3.0,\n",
              " 29.9,\n",
              " 2.6,\n",
              " -5.2,\n",
              " -14.1,\n",
              " 22.5,\n",
              " -5.6,\n",
              " 11.3,\n",
              " -16.6,\n",
              " 3.3,\n",
              " -4.6,\n",
              " 1.1,\n",
              " 47.1,\n",
              " 47.6,\n",
              " 14.2,\n",
              " 6.7,\n",
              " -23.0,\n",
              " -4.0,\n",
              " -6.2,\n",
              " -8.8,\n",
              " -16.3,\n",
              " -13.6,\n",
              " 8.2,\n",
              " 3.8,\n",
              " 2.3,\n",
              " 24.9,\n",
              " 7.7,\n",
              " 12.1,\n",
              " 12.4,\n",
              " 188.8,\n",
              " 3.8,\n",
              " 5.7,\n",
              " 117.8,\n",
              " -1.6,\n",
              " -9.6,\n",
              " -4.1,\n",
              " 23.2,\n",
              " -7.1,\n",
              " -7.4,\n",
              " -4.8,\n",
              " -3.1,\n",
              " 17.5,\n",
              " -1.0,\n",
              " -6.7,\n",
              " -6.8,\n",
              " -1.5,\n",
              " -11.7,\n",
              " -0.9,\n",
              " 3.4,\n",
              " -5.3,\n",
              " 43.4,\n",
              " -21.2,\n",
              " 71.1,\n",
              " 2.7,\n",
              " 12.6,\n",
              " 5.7,\n",
              " 5.8,\n",
              " -1.6,\n",
              " 11.0,\n",
              " 66.3,\n",
              " -0.7,\n",
              " 31.1,\n",
              " -2.5,\n",
              " -7.0,\n",
              " -7.6,\n",
              " 3.6,\n",
              " 5.8,\n",
              " -7.6,\n",
              " -8.4,\n",
              " 14.8,\n",
              " -6.7,\n",
              " -8.4,\n",
              " 23.5,\n",
              " -5.5,\n",
              " -6.3,\n",
              " 113.8,\n",
              " 14.1,\n",
              " -5.1,\n",
              " 4.5,\n",
              " -11.3,\n",
              " -11.9,\n",
              " -1.9,\n",
              " 3.1,\n",
              " -10.6,\n",
              " 97.8,\n",
              " -2.8,\n",
              " -9.6,\n",
              " 26.6,\n",
              " -3.2,\n",
              " 5.5,\n",
              " -8.4,\n",
              " -7.1,\n",
              " 11.8,\n",
              " 8.7,\n",
              " 7.0,\n",
              " 0.5,\n",
              " -16.3,\n",
              " -21.2,\n",
              " -18.3,\n",
              " -12.7,\n",
              " -11.7,\n",
              " -6.6,\n",
              " -10.5,\n",
              " -12.8,\n",
              " -12.5,\n",
              " -1.4,\n",
              " -19.1,\n",
              " -8.0,\n",
              " -5.0,\n",
              " 52.0,\n",
              " -2.0,\n",
              " -13.9,\n",
              " -5.7,\n",
              " 256.9,\n",
              " -21.8,\n",
              " 24.2,\n",
              " -1.7,\n",
              " 77.7,\n",
              " 251.0,\n",
              " 21.4,\n",
              " 36.6,\n",
              " 28.3,\n",
              " 90.0,\n",
              " 139.6,\n",
              " -8.3,\n",
              " -8.0,\n",
              " -8.0,\n",
              " -8.2,\n",
              " -7.8,\n",
              " -8.1,\n",
              " -8.2,\n",
              " -5.5,\n",
              " -7.2,\n",
              " -2.3,\n",
              " -9.8,\n",
              " -8.6,\n",
              " -4.4,\n",
              " -7.5,\n",
              " -5.9,\n",
              " -7.0,\n",
              " 1554.1,\n",
              " -8.6,\n",
              " 9.1,\n",
              " 2.2,\n",
              " -5.8,\n",
              " 0.7,\n",
              " -22.3,\n",
              " -5.9,\n",
              " 13.9,\n",
              " -0.4,\n",
              " -16.5,\n",
              " -13.7,\n",
              " 64.6,\n",
              " -22.5,\n",
              " -11.1,\n",
              " -12.4,\n",
              " -11.0,\n",
              " -12.2,\n",
              " -11.2,\n",
              " 24.8,\n",
              " -7.2,\n",
              " -9.7,\n",
              " -13.2,\n",
              " -10.6,\n",
              " -14.0,\n",
              " -14.4,\n",
              " 12.7,\n",
              " 2.2,\n",
              " -13.4,\n",
              " -12.0,\n",
              " -12.4,\n",
              " -13.4,\n",
              " -11.8,\n",
              " 25.8,\n",
              " 178.1,\n",
              " 8.7,\n",
              " -22.7,\n",
              " 0.4,\n",
              " -5.5,\n",
              " -6.9,\n",
              " 20.2,\n",
              " 61.0,\n",
              " -6.4,\n",
              " 6.1,\n",
              " -8.9,\n",
              " -9.9,\n",
              " -4.5,\n",
              " -4.2,\n",
              " -4.1,\n",
              " 4.1,\n",
              " 6.0,\n",
              " -8.5,\n",
              " 3.9,\n",
              " -22.7,\n",
              " -8.0,\n",
              " 25.4,\n",
              " 2.7,\n",
              " 13.5,\n",
              " -9.4,\n",
              " -2.3,\n",
              " 1.6,\n",
              " 82.2,\n",
              " -4.8,\n",
              " 15.7,\n",
              " -0.9,\n",
              " -0.9,\n",
              " -3.4,\n",
              " 89.6,\n",
              " 1.9,\n",
              " 1.0,\n",
              " -0.2,\n",
              " -0.7,\n",
              " 0.0,\n",
              " -0.2,\n",
              " -0.1,\n",
              " 9.4,\n",
              " -0.1,\n",
              " 0.1,\n",
              " 37.8,\n",
              " 6.7,\n",
              " 134.0,\n",
              " 42.2,\n",
              " 43.3,\n",
              " 5.0,\n",
              " 97.6,\n",
              " 0.5,\n",
              " 1.9,\n",
              " -1.3,\n",
              " -2.8,\n",
              " 5.5,\n",
              " 12.9,\n",
              " 7.3,\n",
              " -0.2,\n",
              " 3.8,\n",
              " -5.5,\n",
              " 3.2,\n",
              " -1.3,\n",
              " 10.6,\n",
              " 11.1,\n",
              " 8.7,\n",
              " 94.4,\n",
              " 5.2,\n",
              " 2.1,\n",
              " 14.4,\n",
              " 113.5,\n",
              " -16.6,\n",
              " 17.2,\n",
              " -10.6,\n",
              " 0.5,\n",
              " 26.9,\n",
              " 15.8,\n",
              " -12.3,\n",
              " -10.3,\n",
              " -10.2,\n",
              " 283.6,\n",
              " 98.4,\n",
              " 78.5,\n",
              " 6.9,\n",
              " 5.2,\n",
              " 10.2,\n",
              " 7.7,\n",
              " 0.5,\n",
              " -8.5,\n",
              " 4.8,\n",
              " 10.3,\n",
              " -7.0,\n",
              " 3.2,\n",
              " 19.4,\n",
              " 15.7,\n",
              " -1.0,\n",
              " 4.4,\n",
              " -10.0,\n",
              " 70.6,\n",
              " -12.7,\n",
              " -12.7,\n",
              " -11.1,\n",
              " -12.7,\n",
              " -7.8,\n",
              " -8.5,\n",
              " -15.9,\n",
              " 4.0,\n",
              " -12.3,\n",
              " 12.1,\n",
              " 48.8,\n",
              " 37.3,\n",
              " -10.2,\n",
              " -14.7,\n",
              " -7.1,\n",
              " -15.1,\n",
              " -18.7,\n",
              " -17.0,\n",
              " -15.4,\n",
              " -8.3,\n",
              " -12.5,\n",
              " -17.9,\n",
              " -12.6,\n",
              " 1.9,\n",
              " -9.6,\n",
              " -12.3,\n",
              " -15.8,\n",
              " -4.3,\n",
              " -14.0,\n",
              " -8.2,\n",
              " 358.1,\n",
              " 74.7,\n",
              " 3.4,\n",
              " -10.0,\n",
              " -9.9,\n",
              " -9.0,\n",
              " -3.9,\n",
              " -11.7,\n",
              " -9.1,\n",
              " -13.9,\n",
              " -1.0,\n",
              " -13.3,\n",
              " -13.8,\n",
              " -8.6,\n",
              " 2.3,\n",
              " -14.8,\n",
              " -13.5,\n",
              " -5.0,\n",
              " -10.3,\n",
              " -11.4,\n",
              " -11.5,\n",
              " -9.9,\n",
              " -15.4,\n",
              " -11.8,\n",
              " -8.3,\n",
              " -12.0,\n",
              " -8.6,\n",
              " -2.8,\n",
              " -14.6,\n",
              " -15.1,\n",
              " -11.5,\n",
              " 90.1,\n",
              " -14.5,\n",
              " -20.6,\n",
              " -15.9,\n",
              " 24.4,\n",
              " 6.6,\n",
              " 1.4,\n",
              " -8.7,\n",
              " 50.8,\n",
              " -14.4,\n",
              " -15.6,\n",
              " -10.2,\n",
              " -15.0,\n",
              " -4.7,\n",
              " 8.3,\n",
              " -24.0,\n",
              " -11.2,\n",
              " -14.0,\n",
              " -0.0,\n",
              " -15.0,\n",
              " -15.7,\n",
              " -12.5,\n",
              " -13.9,\n",
              " -10.3,\n",
              " -6.9,\n",
              " 20.1,\n",
              " -13.6,\n",
              " -13.4,\n",
              " -15.5,\n",
              " -14.7,\n",
              " 171.5,\n",
              " 56.3,\n",
              " -9.4,\n",
              " -7.3,\n",
              " -8.9,\n",
              " 128.0,\n",
              " -6.8,\n",
              " 16.8,\n",
              " -0.3,\n",
              " -8.5,\n",
              " -14.6,\n",
              " -10.5,\n",
              " -10.7,\n",
              " -9.4,\n",
              " -12.9,\n",
              " -2.8,\n",
              " 20.8,\n",
              " 1.1,\n",
              " 13.5,\n",
              " -8.7,\n",
              " 22.0,\n",
              " -23.5,\n",
              " -6.4,\n",
              " -9.3,\n",
              " -15.7,\n",
              " 80.4,\n",
              " -0.4,\n",
              " -11.8,\n",
              " -9.0,\n",
              " -7.6,\n",
              " 31.6,\n",
              " -6.7,\n",
              " 71.1,\n",
              " -8.5,\n",
              " -14.9,\n",
              " -16.0,\n",
              " 48.0,\n",
              " -14.6,\n",
              " -13.0,\n",
              " -15.2,\n",
              " -14.4,\n",
              " -15.8,\n",
              " -13.6,\n",
              " -16.6,\n",
              " -6.0,\n",
              " 0.9,\n",
              " 302.4,\n",
              " -15.9,\n",
              " -0.9,\n",
              " -0.3,\n",
              " 6.6,\n",
              " 83.8,\n",
              " -11.7,\n",
              " -9.3,\n",
              " -10.6,\n",
              " -11.4,\n",
              " -13.6,\n",
              " 11.6,\n",
              " -10.4,\n",
              " -13.5,\n",
              " -18.3,\n",
              " -5.7,\n",
              " 8.8,\n",
              " -9.8,\n",
              " -7.1,\n",
              " -2.7,\n",
              " -5.0,\n",
              " 1.1,\n",
              " -19.3,\n",
              " 85.0,\n",
              " -10.8,\n",
              " 1540.7,\n",
              " -12.5,\n",
              " 11.8,\n",
              " -10.4,\n",
              " -11.2,\n",
              " 18.5,\n",
              " -23.6,\n",
              " 2.2,\n",
              " -6.9,\n",
              " -14.6,\n",
              " -14.0,\n",
              " 167.0,\n",
              " 5.9,\n",
              " 6.1,\n",
              " -8.7,\n",
              " 3.0,\n",
              " -20.1,\n",
              " -1.8,\n",
              " -1.8,\n",
              " -1.0,\n",
              " 2.9,\n",
              " 10.7,\n",
              " -12.3,\n",
              " -0.2,\n",
              " -7.7,\n",
              " 5.8,\n",
              " -3.1,\n",
              " -1.7,\n",
              " -6.2,\n",
              " 7.9,\n",
              " -10.3,\n",
              " -22.4,\n",
              " 28.1,\n",
              " -9.8,\n",
              " -7.3,\n",
              " -6.8,\n",
              " 14.4,\n",
              " 2.8,\n",
              " 10.1,\n",
              " 52.4,\n",
              " 6.7,\n",
              " -2.2,\n",
              " -5.8,\n",
              " 21.2,\n",
              " 3.0,\n",
              " 5.6,\n",
              " 5.2,\n",
              " 3.0,\n",
              " 4.4,\n",
              " 3.7,\n",
              " 5.7,\n",
              " 26.6,\n",
              " 3.3,\n",
              " 41.8,\n",
              " 42.8,\n",
              " 47.8,\n",
              " -4.8,\n",
              " -9.4,\n",
              " 0.7,\n",
              " -4.2,\n",
              " 31.9,\n",
              " 40.0,\n",
              " 9.7,\n",
              " -3.1,\n",
              " -4.5,\n",
              " -14.7,\n",
              " -7.8,\n",
              " -7.6,\n",
              " -4.3,\n",
              " -5.6,\n",
              " 0.6,\n",
              " -6.7,\n",
              " -5.8,\n",
              " 1.6,\n",
              " -3.6,\n",
              " 0.3,\n",
              " 12.9,\n",
              " -5.0,\n",
              " 170.4,\n",
              " -8.9,\n",
              " -6.8,\n",
              " -12.0,\n",
              " -10.1,\n",
              " -9.1,\n",
              " -9.2,\n",
              " -7.7,\n",
              " 72.6,\n",
              " -11.5,\n",
              " -13.6,\n",
              " -10.9,\n",
              " -13.5,\n",
              " -11.9,\n",
              " -6.9,\n",
              " -9.6,\n",
              " -7.8,\n",
              " -4.9,\n",
              " -8.6,\n",
              " -11.6,\n",
              " -11.9,\n",
              " -10.0,\n",
              " -9.3,\n",
              " -9.1,\n",
              " -12.3,\n",
              " -11.0,\n",
              " -10.6,\n",
              " -10.1,\n",
              " -10.1,\n",
              " -11.4,\n",
              " 2.7,\n",
              " 0.2,\n",
              " -8.1,\n",
              " -1.0,\n",
              " -12.4,\n",
              " 93.9,\n",
              " 28.2,\n",
              " -8.2,\n",
              " -7.9,\n",
              " -4.3,\n",
              " 5.3,\n",
              " -9.1,\n",
              " 64.8,\n",
              " 1.3,\n",
              " -4.2,\n",
              " 19.2,\n",
              " -5.4,\n",
              " -2.9,\n",
              " -3.3,\n",
              " 4.8,\n",
              " -5.2,\n",
              " -3.9,\n",
              " 2.1,\n",
              " 10.2,\n",
              " 5.8,\n",
              " -6.7,\n",
              " -5.4,\n",
              " 163.0,\n",
              " -7.3,\n",
              " -7.0,\n",
              " -5.9,\n",
              " -10.8,\n",
              " -11.0,\n",
              " -6.0,\n",
              " -5.0,\n",
              " 88.1,\n",
              " -12.4,\n",
              " -7.0,\n",
              " -6.1,\n",
              " -13.1,\n",
              " -10.7,\n",
              " -11.8,\n",
              " -10.5,\n",
              " -0.5,\n",
              " 4.9,\n",
              " 1.3,\n",
              " 2.5,\n",
              " -5.9,\n",
              " 33.1,\n",
              " 10.2,\n",
              " 5.7,\n",
              " 2.5,\n",
              " 58.8,\n",
              " 5.4,\n",
              " 5.0,\n",
              " 4.1,\n",
              " 0.8,\n",
              " 5.3,\n",
              " 29.5,\n",
              " 66.1,\n",
              " 7.0,\n",
              " 2.4,\n",
              " 9.6,\n",
              " -6.1,\n",
              " 7.2,\n",
              " 14.7,\n",
              " 3.9,\n",
              " 11.2,\n",
              " 9.9,\n",
              " 9.3,\n",
              " 7.8,\n",
              " 188.4,\n",
              " 6.8,\n",
              " 6.5,\n",
              " 126.9,\n",
              " -10.3,\n",
              " -13.1,\n",
              " -11.1,\n",
              " -15.8,\n",
              " -12.6,\n",
              " 12.4,\n",
              " 32.8,\n",
              " -11.8,\n",
              " -6.6,\n",
              " -14.3,\n",
              " 14.4,\n",
              " -13.2,\n",
              " -15.0,\n",
              " -14.3,\n",
              " -13.9,\n",
              " -13.4,\n",
              " 10.0,\n",
              " -11.7,\n",
              " -13.1,\n",
              " 15.9,\n",
              " 71.5,\n",
              " -5.9,\n",
              " -7.9,\n",
              " -11.2,\n",
              " -11.0,\n",
              " 2.3,\n",
              " -10.2,\n",
              " -10.8,\n",
              " -13.3,\n",
              " -11.4,\n",
              " -12.0,\n",
              " -16.3,\n",
              " -11.0,\n",
              " 52.0,\n",
              " -2.4,\n",
              " -10.6,\n",
              " -8.0,\n",
              " -10.2,\n",
              " -7.2,\n",
              " -4.5,\n",
              " -6.6,\n",
              " 0.4,\n",
              " -5.4,\n",
              " -5.2,\n",
              " 6.1,\n",
              " -0.4,\n",
              " 349.4,\n",
              " -13.2,\n",
              " -15.1,\n",
              " 221.3,\n",
              " 0.7,\n",
              " -3.2,\n",
              " 0.2,\n",
              " 36.3,\n",
              " -0.9,\n",
              " -0.4,\n",
              " -1.3,\n",
              " -1.2,\n",
              " -4.5,\n",
              " -5.5,\n",
              " -0.6,\n",
              " 60.5,\n",
              " -23.9,\n",
              " 226.6,\n",
              " 50.8,\n",
              " 48.1,\n",
              " 473.2,\n",
              " 120.1,\n",
              " 43.1,\n",
              " 32.5,\n",
              " 42.7,\n",
              " 1703.9,\n",
              " 81.2,\n",
              " 77.4,\n",
              " 99.1,\n",
              " 66.0,\n",
              " 75.5,\n",
              " 315.1,\n",
              " 10.0,\n",
              " -12.4,\n",
              " 12.0,\n",
              " -0.3,\n",
              " -2.0,\n",
              " 68.4,\n",
              " -5.1,\n",
              " 8.6,\n",
              " 11.7,\n",
              " -2.0,\n",
              " 18.8,\n",
              " 3.9,\n",
              " 26.6,\n",
              " 4.5,\n",
              " 6.4,\n",
              " -0.3,\n",
              " 6.0,\n",
              " 2.4,\n",
              " -4.2,\n",
              " -1.1,\n",
              " 51.6,\n",
              " 0.1,\n",
              " 10.8,\n",
              " 15.6,\n",
              " 1.4,\n",
              " 27.4,\n",
              " 59.6,\n",
              " -7.5,\n",
              " -7.7,\n",
              " -6.4,\n",
              " -4.9,\n",
              " -7.4,\n",
              " -7.1,\n",
              " -7.0,\n",
              " -8.0,\n",
              " -5.4,\n",
              " -7.4,\n",
              " -7.1,\n",
              " -1.8,\n",
              " -8.3,\n",
              " -3.0,\n",
              " -7.2,\n",
              " -4.3,\n",
              " -6.7,\n",
              " 233.0,\n",
              " 4.7,\n",
              " -0.3,\n",
              " -2.2,\n",
              " 38.1,\n",
              " 68.6,\n",
              " -2.1,\n",
              " 7.0,\n",
              " -2.4,\n",
              " -0.3,\n",
              " 18.8,\n",
              " -7.0,\n",
              " -8.7,\n",
              " -2.8,\n",
              " -3.4,\n",
              " -13.3,\n",
              " -8.5,\n",
              " -2.6,\n",
              " -4.7,\n",
              " 9.3,\n",
              " -5.8,\n",
              " -10.9,\n",
              " -11.7,\n",
              " -11.5,\n",
              " -12.1,\n",
              " -12.6,\n",
              " -8.8,\n",
              " 68.5,\n",
              " 12.9,\n",
              " -15.8,\n",
              " -14.0,\n",
              " 3.0,\n",
              " -5.2,\n",
              " -10.4,\n",
              " -8.9,\n",
              " -11.4,\n",
              " -6.7,\n",
              " -9.5,\n",
              " 54.6,\n",
              " 6.9,\n",
              " -9.6,\n",
              " -6.7,\n",
              " -8.0,\n",
              " -8.0,\n",
              " -9.8,\n",
              " 32.5,\n",
              " -9.5,\n",
              " -8.2,\n",
              " 13.3,\n",
              " 64.8,\n",
              " -11.6,\n",
              " -9.7,\n",
              " -9.0,\n",
              " 154.3,\n",
              " -10.6,\n",
              " -7.6,\n",
              " -9.6,\n",
              " -2.9,\n",
              " -8.6,\n",
              " -11.2,\n",
              " -12.6,\n",
              " -7.1,\n",
              " -17.3,\n",
              " 65.4,\n",
              " -10.1,\n",
              " -4.1,\n",
              " -3.5,\n",
              " -4.3,\n",
              " -4.9,\n",
              " -7.3,\n",
              " -0.9,\n",
              " 8.9,\n",
              " -4.4,\n",
              " -7.0,\n",
              " -4.4,\n",
              " -6.5,\n",
              " -7.2,\n",
              " -9.0,\n",
              " 11.6,\n",
              " -3.4,\n",
              " 6.0,\n",
              " 18.6,\n",
              " -7.5,\n",
              " -11.9,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Output을 보면 알겠지만 값이 음수로 나오는 것들이 있다. 이런 경우는 엑셀 파일에서 따로 0으로 처리해 주었다."
      ],
      "metadata": {
        "id": "fy5t_OxaFqVJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "또한 작업 시간을 조정 했을 때, 음수가 나오는 경우가 아주 간혹 있는데 이 경우에는 값을 조정하기 전에 있는 값을 그대로 사용했다."
      ],
      "metadata": {
        "id": "ise3zM1cFytD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# output 값 열을 추가한 뒤 엑셀 파일로 저장\n",
        "test_df.to_excel('step6_preprocessing.xlsx')"
      ],
      "metadata": {
        "id": "ofqT3NnI5R5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EyfePC2E8u3Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
